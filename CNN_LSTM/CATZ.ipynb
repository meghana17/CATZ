{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CATZ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "W1of9OzUPbcN",
        "outputId": "26dfc2c9-980e-4682-c819-d01931bd435b"
      },
      "source": [
        "%cp /content/drive/My\\ Drive/save_weight/*prednet_kitti_weights_train4*  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/*weights_train_delete_training_3_nt*  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/*INITIAL_weights_train*  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/*weights_train_SPECIAL6_2*  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/*test_entire*  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/*prednet_kitti_model*  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/delete_training_model_3_nt.json  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/INITIAL_model.json*  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/training_model_SPECIAL6.json*  /content/\n",
        "%cp /content/drive/My\\ Drive/save_weight/*best_validation*  /content/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/My Drive/save_weight/*weights_train_SPECIAL6_2*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "jhWcnxu1TRBQ",
        "outputId": "b9c01a0e-e606-414a-8e62-79bc4f8f365d"
      },
      "source": [
        "!python3 utils.py\n",
        "!python3 prednet.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0806 15:05:56.251872 140424643852160 deprecation_wrapper.py:119] From utils.py:9: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "Using TensorFlow backend.\n",
            "Downloading catz dataset...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.4M  100 80.4M    0     0  17.0M      0  0:00:04  0:00:04 --:--:-- 17.8M\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "sN0qRdcFPk7W",
        "outputId": "0b5af9e6-bc59-4c38-8445-a62d7a6fa33f"
      },
      "source": [
        "from utils import *\n",
        "   \n",
        "def perceptual_distance(y_true, y_pred):\n",
        "    # Muliply by 225 because normalization divides by 225.\n",
        "    # Taking the last output of the LSTM\n",
        "\n",
        "    y_pred  =denormalize (y_pred)\n",
        "    y_true  =denormalize (y_true) \n",
        "    #print(\"y_true_1\",K.int_shape(y_pred1))\n",
        "    #print(\"y_true_1\",K.int_shape(y_true1))\n",
        "    \n",
        "    rmean = (y_true[:, :, :, 0] + y_pred[:, :, :, 0]) / 2\n",
        "    r = y_true[:, :, :, 0] - y_pred[:, :, :, 0]\n",
        "    g = y_true[:, :, :, 1] - y_pred[:, :, :, 1]\n",
        "    b = y_true[:, :, :, 2] - y_pred[:, :, :, 2]\n",
        "\n",
        "    return K.mean(K.sqrt((((512+rmean)*r*r)/256) + 4*g*g + (((767-rmean)*b*b)/256)))\n",
        "\n",
        "from keras.models import load_model\n",
        "new_model = load_model('/content/best_validation.hdf5',custom_objects={'perceptual_distance':perceptual_distance,'loss':loss}) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0806 15:08:55.680596 139947945498496 deprecation_wrapper.py:119] From /content/utils.py:9: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "Using TensorFlow backend.\n",
            "W0806 15:09:04.326379 139947945498496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0806 15:09:04.334940 139947945498496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0806 15:09:04.343795 139947945498496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0806 15:09:04.742151 139947945498496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0806 15:09:04.765600 139947945498496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0806 15:09:05.817245 139947945498496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "W0806 15:09:06.845022 139947945498496 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0806 15:09:10.249389 139947945498496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0806 15:09:10.832868 139947945498496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "alpha 1\n",
            "(?, 96, 96, 3)\n",
            "SHAPE_OF_GDL_LOSS (?, 96, 96)\n",
            "y_true (?, ?, ?, ?)\n",
            "y_pred (?, 96, 96, 3)\n",
            "SHAPE_OF_LOSS (None, 96, 96)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0806 15:09:11.183794 139947945498496 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUZHe-ObQE4u"
      },
      "source": [
        "import hickle as hkl\n",
        "from keras.models import Model, model_from_json\n",
        "import os\n",
        "from prednet import *\n",
        "\n",
        "test_file    = os.path.join('/content/test_entire2_X_test.hkl')\n",
        "test_sources = os.path.join('/content/test_entire2_sources_test.hkl')\n",
        "\n",
        "\n",
        "#nt = 4 for training but for training no of images = 5\n",
        "weights_file = os.path.join('/content/prednet_kitti_weights_train4.hdf5')\n",
        "json_file    = os.path.join('/content/prednet_kitti_model.json')\n",
        "\n",
        "\n",
        "#nt = 3 for training but for training no of images = 6,epoch = 50 ,samples_per_epoch = 2433*4, layer_dimension =64,128,256\n",
        "\n",
        "weights_file2 = os.path.join('/content/weights_train_delete_training_3_nt.hdf5')\n",
        "json_file2 = os.path.join('/content/delete_training_model_3_nt.json')\n",
        "\n",
        "\n",
        "weights_file3 = os.path.join('/content/INITIAL_weights_train.hdf5')\n",
        "json_file3 = os.path.join('/content/INITIAL_model.json')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights_file4 = os.path.join('/content/weights_train_SPECIAL6_2.hdf5')\n",
        "json_file4    = os.path.join('/content/training_model_SPECIAL6.json')\n",
        "\n",
        "\n",
        "\n",
        "sources = hkl.load(test_sources)\n",
        "X_array      = hkl.load(test_file)\n",
        "\n",
        "\n",
        "nt = 4\n",
        "# Load trained model\n",
        "f = open(json_file, 'r')\n",
        "json_string = f.read()\n",
        "f.close()\n",
        "train_model = model_from_json(json_string, custom_objects = {'PredNet': PredNet})\n",
        "train_model.load_weights(weights_file)\n",
        "# Create testing model (to output predictions)\n",
        "layer_config = train_model.layers[1].get_config()\n",
        "layer_config['output_mode'] = 'prediction'\n",
        "data_format = layer_config['data_format'] if 'data_format' in layer_config else layer_config['dim_ordering']\n",
        "test_prednet = PredNet(weights=train_model.layers[1].get_weights(), **layer_config)\n",
        "input_shape = list(train_model.layers[0].batch_input_shape[1:])\n",
        "input_shape[0] = nt\n",
        "inputs = Input(shape=tuple(input_shape))\n",
        "predictions = test_prednet(inputs)\n",
        "test_model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nt = 3\n",
        "# Load trained model\n",
        "f = open(json_file2, 'r')\n",
        "json_string2 = f.read()\n",
        "f.close()\n",
        "train_model2 = model_from_json(json_string2, custom_objects = {'PredNet': PredNet})\n",
        "train_model2.load_weights(weights_file2)\n",
        "\n",
        "# Create testing model (to output predictions)\n",
        "layer_config2 = train_model2.layers[1].get_config()\n",
        "layer_config2['output_mode'] = 'prediction'\n",
        "data_format2 = layer_config2['data_format'] if 'data_format' in layer_config2 else layer_config2['dim_ordering']\n",
        "test_prednet2 = PredNet(weights=train_model2.layers[1].get_weights(), **layer_config2)\n",
        "input_shape2 = list(train_model2.layers[0].batch_input_shape[1:])\n",
        "input_shape2[0] = nt\n",
        "inputs2 = Input(shape=tuple(input_shape2))\n",
        "predictions2 = test_prednet2(inputs2)\n",
        "test_model2 = Model(inputs=inputs2, outputs=predictions2)\n",
        "\n",
        "nt = 3\n",
        "\n",
        "# Load trained model\n",
        "f = open(json_file3, 'r')\n",
        "json_string3 = f.read()\n",
        "f.close()\n",
        "train_model3 = model_from_json(json_string3, custom_objects = {'PredNet': PredNet})\n",
        "train_model3.load_weights(weights_file3)\n",
        "\n",
        "\n",
        "# Create testing model (to output predictions)\n",
        "layer_config3 = train_model3.layers[1].get_config()\n",
        "layer_config3['output_mode'] = 'prediction'\n",
        "data_format3 = layer_config3['data_format'] if 'data_format' in layer_config3 else layer_config3['dim_ordering']\n",
        "test_prednet3 = PredNet(weights=train_model3.layers[1].get_weights(), **layer_config3)\n",
        "input_shape3 = list(train_model3.layers[0].batch_input_shape[1:])\n",
        "input_shape3[0] = nt\n",
        "inputs3 = Input(shape=tuple(input_shape3))\n",
        "predictions3 = test_prednet3(inputs3)\n",
        "test_model3 = Model(inputs=inputs3, outputs=predictions3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nt = 3\n",
        "\n",
        "f = open(json_file4, 'r')\n",
        "json_string4 = f.read()\n",
        "f.close()\n",
        "train_model4 = model_from_json(json_string4, custom_objects = {'PredNet': PredNet})\n",
        "train_model4.load_weights(weights_file4)\n",
        "\n",
        "\n",
        "# Create testing model (to output predictions)\n",
        "layer_config4 = train_model4.layers[1].get_config()\n",
        "layer_config4['output_mode'] = 'prediction'\n",
        "data_format4 = layer_config4['data_format'] if 'data_format' in layer_config4 else layer_config4['dim_ordering']\n",
        "test_prednet4 = PredNet(weights=train_model4.layers[1].get_weights(), **layer_config4)\n",
        "input_shape4 = list(train_model4.layers[0].batch_input_shape[1:])\n",
        "input_shape4[0] = nt\n",
        "inputs4 = Input(shape=tuple(input_shape4))\n",
        "predictions4 = test_prednet4(inputs4)\n",
        "test_model4 = Model(inputs=inputs4, outputs=predictions4)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eH-iD7sYTta"
      },
      "source": [
        "def list_start(sources):\n",
        "  \n",
        "  start_list =[]\n",
        "  for i in range(len(sources)-1):\n",
        "    if(sources[i]!=sources[i+1]):\n",
        "      start = i+1\n",
        "      start_list.append(start)\n",
        "  start_list.append(len(sources)-1)\n",
        "  return start_list  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtWoXhigYpAl"
      },
      "source": [
        "def find_closest_img_to_op(input_imgs,output_images):\n",
        "   percep0 = perceptual_distance(input_imgs[:,0,:,:,:],output_images)\n",
        "   percep1 = perceptual_distance(input_imgs[:,1,:,:,:],output_images)\n",
        "   percep2 = perceptual_distance(input_imgs[:,2,:,:,:],output_images)\n",
        "   percep3 = perceptual_distance(input_imgs[:,3,:,:,:],output_images)\n",
        "   percep4 = perceptual_distance(input_imgs[:,4,:,:,:],output_images)\n",
        "\n",
        "   return np.argmin(np.array((percep0,percep1,percep2,percep3,percep4)))\n",
        "\n",
        "def my_predict(img_list):\n",
        "    \"\"\"A generator that returns 5 images plus a result image\"\"\"\n",
        "    \"\" \" Returns a input_images of shape(batch_size,no_of_time_seq,height,width,depth) \"\"\"\n",
        "\n",
        "    cat_dirs = img_list\n",
        "    \n",
        "    counter = 0\n",
        "    j=0\n",
        "    percep_list =[]\n",
        "   \n",
        "    possible_starts  =  list_start(sources)\n",
        "    \n",
        "    \n",
        "    while True:\n",
        "        input_images = np.zeros((1,5 ,width, height,3 ),dtype='float32')\n",
        "        aug_input_image =np.zeros((1,5 ,width, height,3 ),dtype='float32')\n",
        "        output_images   = np.zeros((1,width, height, 3),dtype='float32')\n",
        "        input_images2   = np.zeros((1,width, height, 3),dtype='float32')\n",
        "        \n",
        "        X_all = np.zeros((1,3 ,96, 96,3 ),dtype='float32')\n",
        "        X_all2 = np.zeros((1,3 ,96, 96,3 ),dtype='float32')  \n",
        "        \n",
        "          \n",
        "        if (counter+batch_size >= len(cat_dirs)):\n",
        "            counter = 0\n",
        "            j       = 0\n",
        "       \n",
        "        for i in range(batch_size):\n",
        "            end  =  possible_starts[j]\n",
        "            start =  possible_starts[j]-3\n",
        "            input_imgs = glob.glob(cat_dirs[counter + i] + \"/cat_[0-5]*\")\n",
        "            input_images = np.expand_dims(np.array([normalized(np.array(Image.open(img))) for img in sorted(input_imgs)]),axis=0)\n",
        "            act_out =np.array(Image.open(cat_dirs[counter + i] + \"/cat_result.jpg\"))\n",
        "            act_out =normalized(act_out)\n",
        "            output_images    = np.expand_dims(act_out, axis=0)\n",
        "            \n",
        "            index = find_closest_img_to_op(input_images,output_images)\n",
        "            input_images2    = input_images[:,index,:,:,:]\n",
        "\n",
        "            \n",
        "            #Model1\n",
        "            y1 = new_model.predict([input_images,input_images2])\n",
        "            percep1 = perceptual_distance(output_images,y1)\n",
        "            \n",
        "            #Model2\n",
        "            \n",
        "            end  =  possible_starts[j]\n",
        "            start =  possible_starts[j]-3\n",
        "            X_all2 = np.expand_dims(normalized(X_array[start:end]),axis=0)    \n",
        "            \n",
        "            \n",
        "            X_hat  =test_model3.predict(X_all2)\n",
        "            percep2  = perceptual_distance(X_hat[:,-1,:,:,:], X_all2[:,-1,:,:,:])\n",
        "           \n",
        "            #Model3\n",
        "            X_hat2   =test_model2.predict(X_all2)\n",
        "            percep3  = perceptual_distance(X_hat2[:,-1,:,:,:], X_all2[:,-1,:,:,:])\n",
        "            \n",
        "            #Model5\n",
        "            \n",
        "            X_hat4   =test_model4.predict(X_all2)\n",
        "            percep5  = perceptual_distance(X_hat4[:,-1,:,:,:], X_all2[:,-1,:,:,:])\n",
        "           \n",
        "            \n",
        "            \n",
        "            #Model4\n",
        "            start =  possible_starts[j]-4\n",
        "            X_all = np.expand_dims(normalized(X_array[start:end]),axis=0)\n",
        "            X_hat3   =test_model.predict(X_all)\n",
        "            percep4  = perceptual_distance(X_hat3[:,-1,:,:,:], X_all[:,-1,:,:,:])\n",
        "            \n",
        "           \n",
        "            \n",
        "            start =  possible_starts[j]-3\n",
        "            \n",
        "           \n",
        "            per_list = [percep1,percep2,percep3,percep4,percep5]\n",
        "            percep_list.append(min(per_list))\n",
        "            print(sources[start],min(per_list),percep1,percep2,percep3,percep4,percep5)\n",
        "            \n",
        "            \n",
        "            \n",
        "            j = (j+1)\n",
        "                 \n",
        "        counter += batch_size\n",
        "        yield percep_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "vj3LuFvNEJp9",
        "outputId": "afc0bfe6-398c-4420-e5cb-641de22fb591"
      },
      "source": [
        "!python3 utils.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0806 15:09:55.200101 140063178397568 deprecation_wrapper.py:119] From utils.py:9: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytBmEL3hZoNh"
      },
      "source": [
        "from utils import *\n",
        "\n",
        "co =0\n",
        "for i in my_predict(f7(sources)):\n",
        "  co +=batch_size\n",
        "  \n",
        "  if(co >= len(val_list)):\n",
        "    break\n",
        "  \n",
        "print (sum(i)/len(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfkrBsB23HH-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}